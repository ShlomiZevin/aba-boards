<!DOCTYPE html>
<html lang="he" dir="rtl">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>◊ì◊ô◊†◊ï</title>

  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background: transparent;
      height: 100vh;
      overflow: hidden;
      position: relative;
    }

    /* Mobile audio unlock overlay */
    .audio-unlock {
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: rgba(0,0,0,0.6);
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      z-index: 100;
      gap: 15px;
    }

    .audio-unlock.hidden {
      display: none;
    }

    .unlock-text {
      color: white;
      font-size: 14px;
      text-align: center;
      max-width: 90%;
      line-height: 1.4;
      text-shadow: 0 1px 3px rgba(0,0,0,0.5);
    }

    .unlock-btn {
      background: linear-gradient(135deg, #4CAF50 0%, #45a049 100%);
      border: none;
      border-radius: 50%;
      width: 70px;
      height: 70px;
      font-size: 28px;
      color: white;
      cursor: pointer;
      box-shadow: 0 4px 20px rgba(0,0,0,0.4);
      display: flex;
      align-items: center;
      justify-content: center;
      animation: pulseUnlock 2s ease-in-out infinite;
    }

    @keyframes pulseUnlock {
      0%, 100% { transform: scale(1); box-shadow: 0 4px 20px rgba(0,0,0,0.4); }
      50% { transform: scale(1.1); box-shadow: 0 6px 30px rgba(76, 175, 80, 0.6); }
    }

    /* Dino image - centered in page */
    .avatar-image {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      max-width: 95%;
      max-height: 85%;
      object-fit: contain;
    }

    /* Mic button - bottom right corner near dino's hand */
    .record-btn {
      position: absolute;
      bottom: 12px;
      right: 12px;
      width: 48px;
      height: 48px;
      border-radius: 50%;
      border: 3px solid rgba(255,255,255,0.9);
      cursor: pointer;
      font-size: 22px;
      transition: all 0.2s ease;
      display: flex;
      align-items: center;
      justify-content: center;
      box-shadow: 0 4px 15px rgba(0,0,0,0.4);
      z-index: 50;
    }

    .record-btn.idle {
      background: linear-gradient(135deg, #4CAF50 0%, #45a049 100%);
    }

    .record-btn.recording {
      background: linear-gradient(135deg, #f44336 0%, #d32f2f 100%);
      animation: pulse 1s infinite;
      border-color: rgba(255,255,255,1);
    }

    .record-btn.disabled {
      background: linear-gradient(135deg, #9e9e9e 0%, #757575 100%);
      cursor: not-allowed;
    }

    .record-btn:hover:not(.disabled) {
      transform: scale(1.1);
    }

    .record-btn:active:not(.disabled) {
      transform: scale(0.95);
    }

    @keyframes pulse {
      0%, 100% { box-shadow: 0 0 0 0 rgba(244, 67, 54, 0.6); }
      50% { box-shadow: 0 0 0 12px rgba(244, 67, 54, 0); }
    }

    /* Status display - above mic button in bottom right */
    .status-display {
      position: absolute;
      bottom: 65px;
      right: 8px;
      font-size: 10px;
      font-weight: bold;
      color: white;
      background: rgba(0,0,0,0.6);
      padding: 4px 10px;
      border-radius: 12px;
      z-index: 50;
      backdrop-filter: blur(3px);
      white-space: nowrap;
    }

    .status-display:empty {
      display: none;
    }

    /* Speech bubble - above the mic area */
    .speech-bubble {
      position: absolute;
      top: 10%;
      left: 10px;
      right: 10px;
      background: rgba(255,255,255,0.95);
      padding: 8px 12px;
      border-radius: 12px;
      font-size: 11px;
      color: #333;
      opacity: 0;
      transition: opacity 0.3s ease;
      z-index: 40;
      text-align: center;
      max-height: 50px;
      overflow: hidden;
      line-height: 1.3;
      box-shadow: 0 3px 15px rgba(0,0,0,0.2);
    }

    .speech-bubble.visible {
      opacity: 1;
    }

  </style>
</head>
<body>
  <!-- Mobile audio unlock overlay -->
  <div class="audio-unlock" id="audioUnlock">
    <div class="unlock-text">◊õ◊ì◊ô ◊ú◊ì◊ë◊® ◊¢◊ù ◊ì◊ô◊†◊ï ◊ú◊ó◊• ◊¢◊ú ◊î◊õ◊§◊™◊ï◊®</div>
    <button class="unlock-btn" id="unlockBtn">üé§</button>
  </div>

  <!-- Dino centered in page -->
  <img id="avatarImage" class="avatar-image" src="./avatar/dinosaur/mouth_0.png" onerror="this.style.display='none'">

  <!-- Mic button top right -->
  <button class="record-btn idle" id="recordBtn">üé§</button>

  <!-- Status next to mic -->
  <div class="status-display" id="status"></div>

  <!-- Speech bubble -->
  <div class="speech-bubble" id="speechBubble"></div>

  <audio id="audioPlayer"></audio>

  <script>
    const API_URL = 'https://avatar-server-1018338671074.me-west1.run.app/api';
    //const API_URL = 'http://localhost:3001/api';
    const MOUTH_SHAPE_COUNT = 6;
    const LIP_SYNC_METHOD = 'timestamps';

    // Get kid name from URL parameter - server will fetch full kid data from Firestore
    const urlParams = new URLSearchParams(window.location.search);
    const kidName = urlParams.get('kid');

    if (kidName) {
      console.log('Kid ID from URL:', kidName, '(server will fetch full context)');
    } else {
      console.log('No kid parameter provided');
    }

    // Generate or retrieve user ID for conversation tracking
    const USER_ID = localStorage.getItem('dinoUserId') || (() => {
      const id = 'user_' + Math.random().toString(36).substr(2, 9);
      localStorage.setItem('dinoUserId', id);
      return id;
    })();

    // DOM elements
    const status = document.getElementById('status');
    const audioPlayer = document.getElementById('audioPlayer');
    const speechBubble = document.getElementById('speechBubble');
    const avatarImage = document.getElementById('avatarImage');
    const recordBtn = document.getElementById('recordBtn');
    const audioUnlock = document.getElementById('audioUnlock');
    const unlockBtn = document.getElementById('unlockBtn');

    // Audio unlock for mobile browsers
    let audioUnlocked = false;

    // Tiny valid MP3 file (silence) - needed to unlock HTML audio element
    const SILENT_MP3 = 'data:audio/mp3;base64,SUQzBAAAAAABEVRYWFgAAAAtAAADY29tbWVudABCaWdTb3VuZEJhbmsuY29tIC8gTGFTb25vdGhlcXVlLm9yZwBURU5DAAAAHQAAA1NvZnR3YXJlAExhdmY1Ny44My4xMDAA//tQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAWGluZwAAAA8AAAACAAABhgC7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7u7//////////////////////////////////////////////////////////////////8AAAAATGF2YzU3LjEwAAAAAAAAAAAAAAAAJAAAAAAAAAAAAYZN3pOGAAAAAAD/+9DEAAAFwANf9AAAIjsrLbzEgAgCgKA59ynOf9QhCEIQhCE3/E/6gfB8HwfPKHP4nz58H36gc/lAQBN/4Pn/gh//y4f/E+X8QBH/lzn////5QEAQf4nz////KAgHz4f////+IBg+D/E/+UOfy4PvygIf8T4nxPlxPlznOc5/8oCAIAgD4f//lAQ/5c//+sHwf8T5c5/y4f5cH+IUHD4Pg+cT/y5/xAEHOc/+D58uH+f//6wf/5cP/4gCH/lHE+J8//qBz/E+X8T4nwfP/+IAAAAB//vQxKsAAADSAAAAAAAAANIAAAAAMQU1FMy4xMDBVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV';

    async function unlockAudio() {
      try {
        // Request microphone permission first (while we have user gesture)
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        // Stop it immediately - we just needed to get permission
        stream.getTracks().forEach(track => track.stop());

        // Play silent MP3 through the actual audio element to unlock it
        audioPlayer.src = SILENT_MP3;
        audioPlayer.volume = 0.1;
        await audioPlayer.play();
        audioPlayer.pause();
        audioPlayer.volume = 1.0;

        // CRITICAL: Initialize and unlock Web Audio API for streaming playback
        const ctx = initPlaybackAudioContext();
        if (ctx.state === 'suspended') {
          await ctx.resume();
        }

        // Create and play a very short silent buffer to fully unlock Web Audio API
        const silentBuffer = ctx.createBuffer(1, 1, 22050);
        const silentSource = ctx.createBufferSource();
        silentSource.buffer = silentBuffer;
        silentSource.connect(ctx.destination);
        silentSource.start(0);

        audioUnlocked = true;
        audioUnlock.classList.add('hidden');
      } catch (err) {
        console.error('Audio unlock error:', err);
        // Hide anyway so user can try
        audioUnlock.classList.add('hidden');
      }
    }

    // Check if mobile
    function isMobile() {
      return /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
    }

    // Show unlock overlay only on mobile
    if (!isMobile()) {
      audioUnlock.classList.add('hidden');
      audioUnlocked = true;
    }

    unlockBtn.addEventListener('click', unlockAudio);

    // Mouth shape images
    let mouthImages = [];

    // Thinking animation images
    let thinkingImages = [];
    let thinkingAnimationInterval = null;
    let currentThinkingFrame = 0;
    const THINKING_IMAGE_COUNT = 6;
    const THINKING_ANIMATION_SPEED = 300; // ms between frames

    // Listening animation images
    let listeningImages = [];
    let listeningAnimationInterval = null;
    const LISTENING_ANIMATION_SPEED = 250; // ms between frames

    // Recording state
    let mediaRecorder = null;
    let audioChunks = [];
    let isRecording = false;
    let audioContext = null;
    let analyser = null;
    let silenceTimer = null;
    let hasSpeechStarted = false;

    // Silence detection settings
    const SILENCE_THRESHOLD = 5;
    const SILENCE_DURATION = 1500;
    const MIN_RECORDING_TIME = 500;

    // Load mouth images
    async function loadMouthImages() {
      const images = [];

      for (let i = 0; i < MOUTH_SHAPE_COUNT; i++) {
        const img = new Image();
        img.src = `./avatar/dinosaur/mouth_${i}.png`;

        try {
          await new Promise((resolve, reject) => {
            img.onload = resolve;
            img.onerror = reject;
          });
          images.push(img.src);
        } catch {
          break;
        }
      }

      if (images.length > 0) {
        mouthImages = images;
        avatarImage.src = images[0];
        avatarImage.style.display = 'block';
      }
    }

    // Load thinking images
    async function loadThinkingImages() {
      const images = [];

      for (let i = 0; i < THINKING_IMAGE_COUNT; i++) {
        const img = new Image();
        img.src = `./avatar/dinosaur/thinking_${i}.png`;

        try {
          await new Promise((resolve, reject) => {
            img.onload = resolve;
            img.onerror = reject;
          });
          images.push(img.src);
        } catch {
          break;
        }
      }

      if (images.length > 0) {
        thinkingImages = images;
      }
    }

    // Load listening images (generic - loads as many as exist)
    async function loadListeningImages() {
      const images = [];
      let i = 0;

      // Keep trying to load images until one fails
      while (true) {
        const img = new Image();
        img.src = `./avatar/dinosaur/listening_${i}.png`;

        try {
          await new Promise((resolve, reject) => {
            img.onload = resolve;
            img.onerror = reject;
          });
          images.push(img.src);
          i++;
        } catch {
          break;
        }
      }

      if (images.length > 0) {
        listeningImages = images;
        console.log(`Loaded ${listeningImages.length} listening images`);
      }
    }

    // Start thinking animation (random order)
    function startThinkingAnimation() {
      if (thinkingImages.length === 0) return;

      const randomIndex = Math.floor(Math.random() * thinkingImages.length);
      avatarImage.src = thinkingImages[randomIndex];

      thinkingAnimationInterval = setInterval(() => {
        const nextIndex = Math.floor(Math.random() * thinkingImages.length);
        avatarImage.src = thinkingImages[nextIndex];
      }, THINKING_ANIMATION_SPEED);
    }

    // Stop thinking animation
    function stopThinkingAnimation() {
      if (thinkingAnimationInterval) {
        clearInterval(thinkingAnimationInterval);
        thinkingAnimationInterval = null;
      }
      // Return to default mouth shape
      if (mouthImages.length > 0) {
        avatarImage.src = mouthImages[0];
      }
    }

    // Start listening animation (random order)
    function startListeningAnimation() {
      if (listeningImages.length === 0) return;

      const randomIndex = Math.floor(Math.random() * listeningImages.length);
      avatarImage.src = listeningImages[randomIndex];

      listeningAnimationInterval = setInterval(() => {
        const nextIndex = Math.floor(Math.random() * listeningImages.length);
        avatarImage.src = listeningImages[nextIndex];
      }, LISTENING_ANIMATION_SPEED);
    }

    // Stop listening animation
    function stopListeningAnimation() {
      if (listeningAnimationInterval) {
        clearInterval(listeningAnimationInterval);
        listeningAnimationInterval = null;
      }
      // Return to default mouth shape
      if (mouthImages.length > 0) {
        avatarImage.src = mouthImages[0];
      }
    }

    // Animate mouth
    function setMouthShape(shapeIndex) {
      if (mouthImages[shapeIndex]) {
        avatarImage.src = mouthImages[shapeIndex];
      }
    }

    // Play lip-sync animation
    function playLipSync(lipSyncData) {
      lipSyncData.forEach(cue => {
        setTimeout(() => {
          setMouthShape(cue.shapeIndex);
        }, cue.start * 1000);
      });

      const lastCue = lipSyncData[lipSyncData.length - 1];
      if (lastCue) {
        setTimeout(() => {
          setMouthShape(0);
        }, lastCue.end * 1000);
      }
    }

    // Speech bubble
    function showSpeechBubble(text) {
      speechBubble.textContent = text;
      speechBubble.classList.add('visible');
    }

    function hideSpeechBubble() {
      speechBubble.classList.remove('visible');
    }

    // Set button state
    function setButtonState(state) {
      recordBtn.className = `record-btn ${state}`;
      if (state === 'idle') {
        recordBtn.textContent = 'üé§';
        recordBtn.disabled = false;
      } else if (state === 'recording') {
        recordBtn.textContent = '‚èπÔ∏è';
        recordBtn.disabled = false;
      } else if (state === 'disabled') {
        recordBtn.textContent = '‚è≥';
        recordBtn.disabled = true;
      }
    }

    // Toggle recording
    async function toggleRecording() {
      if (isRecording) {
        stopRecording();
      } else {
        try {
          // Initialize playback context and unlock it if needed
          const ctx = initPlaybackAudioContext();
          console.log('Record button clicked, AudioContext state:', ctx.state);

          if (ctx.state === 'suspended') {
            await ctx.resume();
            console.log('AudioContext resumed for beep, new state:', ctx.state);
          }

          // Always play a brief beep sound when starting to record (provides audio feedback)
          const beepBuffer = ctx.createBuffer(1, ctx.sampleRate * 0.1, ctx.sampleRate);
          const data = beepBuffer.getChannelData(0);
          for (let i = 0; i < data.length; i++) {
            data[i] = Math.sin(2 * Math.PI * 800 * i / ctx.sampleRate) * 0.3;
          }
          const beepSource = ctx.createBufferSource();
          beepSource.buffer = beepBuffer;
          beepSource.connect(ctx.destination);
          beepSource.start(0);
          console.log('Beep started');

          if (!audioUnlocked) {
            audioUnlocked = true;
          }

          const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
          mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
          audioChunks = [];
          hasSpeechStarted = false;

          // Setup audio analysis
          audioContext = new AudioContext();
          if (audioContext.state === 'suspended') {
            await audioContext.resume();
          }

          const source = audioContext.createMediaStreamSource(stream);
          analyser = audioContext.createAnalyser();
          analyser.fftSize = 512;
          source.connect(analyser);

          const dataArray = new Uint8Array(analyser.frequencyBinCount);
          const recordingStartTime = Date.now();

          // Monitor audio levels
          function checkAudioLevel() {
            if (!isRecording) return;

            analyser.getByteFrequencyData(dataArray);
            const average = dataArray.reduce((a, b) => a + b) / dataArray.length;

            if (average > SILENCE_THRESHOLD) {
              hasSpeechStarted = true;
              status.textContent = '◊û◊ì◊ë◊®...';
              if (silenceTimer) {
                clearTimeout(silenceTimer);
                silenceTimer = null;
              }
            } else if (hasSpeechStarted && Date.now() - recordingStartTime > MIN_RECORDING_TIME) {
              status.textContent = '◊©◊ß◊ò...';
              if (!silenceTimer) {
                silenceTimer = setTimeout(() => {
                  stopRecording();
                }, SILENCE_DURATION);
              }
            } else {
              status.textContent = '◊û◊ß◊©◊ô◊ë...';
            }

            requestAnimationFrame(checkAudioLevel);
          }

          mediaRecorder.ondataavailable = (e) => {
            audioChunks.push(e.data);
          };

          mediaRecorder.onstop = async () => {
            stream.getTracks().forEach(track => track.stop());
            stopListeningAnimation();
            if (audioContext) {
              audioContext.close();
              audioContext = null;
            }
            if (silenceTimer) {
              clearTimeout(silenceTimer);
              silenceTimer = null;
            }

            if (hasSpeechStarted && audioChunks.length > 0) {
              const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
              await sendAudioToServer(audioBlob);
            } else {
              status.textContent = '◊ú◊ê ◊©◊û◊¢◊™◊ô';
              setTimeout(() => {
                status.textContent = '';
              }, 2000);
              setButtonState('idle');
            }
          };

          mediaRecorder.start(100);
          setButtonState('recording');
          status.textContent = '◊û◊ß◊©◊ô◊ë...';
          isRecording = true;
          startListeningAnimation();
          checkAudioLevel();

        } catch (err) {
          console.error('Microphone error:', err);
          status.textContent = '◊ê◊ô◊ü ◊û◊ô◊ß◊®◊ï◊§◊ï◊ü';
        }
      }
    }

    function stopRecording() {
      if (mediaRecorder && isRecording) {
        mediaRecorder.stop();
        isRecording = false;
      }
    }

    // ===== STREAMING AUDIO PLAYBACK =====

    // Audio playback state for streaming
    let playbackAudioContext = null;
    let audioChunksQueue = [];
    let isPlayingQueue = false;
    let currentLipSyncTimeouts = [];
    let cumulativeDuration = 0;
    let streamingComplete = false;
    const MIN_BUFFER_CHUNKS = 1; // Start immediately - each chunk is now a complete sentence

    // Initialize audio context for streaming playback (for beep only)
    function initPlaybackAudioContext() {
      if (!playbackAudioContext || playbackAudioContext.state === 'closed') {
        playbackAudioContext = new AudioContext();
      }
      return playbackAudioContext;
    }

    // Queue audio chunk - play them sequentially via HTML audio element
    async function queueAudioChunk(audioBase64, lipSyncData, sentenceIndex, text) {
      try {
        const queueLengthBefore = audioChunksQueue.length;
        console.log(`üì• Received sentence ${sentenceIndex}, queue length before:`, queueLengthBefore);

        // Store chunk with its lip sync data, sentence index, and text
        audioChunksQueue.push({
          audioBase64,
          lipSyncData,
          sentenceIndex,
          text,
          startTime: cumulativeDuration
        });

        console.log(`‚úì Queued sentence ${sentenceIndex}, total in queue: ${audioChunksQueue.length}`);

        // Start playing queue if not already playing
        if (!isPlayingQueue) {
          console.log('‚ñ∂Ô∏è  Starting playback immediately');
          playNextAudioChunk();
        } else {
          console.log('‚è∏Ô∏è  Already playing, chunk will play after current one finishes');
        }
      } catch (err) {
        console.error('Error queueing audio chunk:', err);
      }
    }

    // Play next chunk from queue sequentially
    async function playNextAudioChunk() {
      if (audioChunksQueue.length === 0) {
        // No chunks to play - mark as not playing so next chunk will start playback
        isPlayingQueue = false;
        console.log('‚è∏Ô∏è  Queue empty, stopped playing');

        // If streaming is complete, clean up UI
        if (streamingComplete) {
          console.log('‚úÖ Streaming complete, cleaning up');
          hideSpeechBubble();
          setMouthShape(0);
          status.textContent = '';
          setButtonState('idle');
        }
        return;
      }

      // Mark as playing BEFORE we start
      isPlayingQueue = true;
      const chunk = audioChunksQueue.shift();

      console.log(`üîä Playing sentence ${chunk.sentenceIndex}, remaining in queue: ${audioChunksQueue.length}`);

      // Set audio source first - Opus format
      audioPlayer.src = `data:audio/ogg;base64,${chunk.audioBase64}`;

      console.log('Set audio src, data length:', chunk.audioBase64.length, 'lip sync events:', chunk.lipSyncData?.length || 0);

      // Wait for audio to be ready to play
      return new Promise((resolve) => {
        let hasShownText = false;
        let hasStartedPlaying = false;

        // Use timeupdate to detect when audio is TRULY playing (currentTime advancing)
        audioPlayer.ontimeupdate = () => {
          if (hasShownText) return;
          if (audioPlayer.currentTime > 0.05) { // Wait until at least 50ms of audio has played
            hasShownText = true;

            console.log('Audio confirmed playing, currentTime:', audioPlayer.currentTime);

            // Update speech bubble when audio is TRULY playing
            if (chunk.text) {
              // Show only the current sentence
              showSpeechBubble(chunk.text);
            }

            // Animate lip sync for this chunk (adjust for the small delay)
            if (chunk.lipSyncData && chunk.lipSyncData.length > 0) {
              playLipSync(chunk.lipSyncData);
            }
          }
        };

        // When audio can start playing, trigger play
        audioPlayer.oncanplay = async () => {
          if (hasStartedPlaying) return;
          hasStartedPlaying = true;

          console.log('Audio ready, starting playback');

          try {
            await audioPlayer.play();
            console.log('Play() returned successfully');
          } catch (playErr) {
            console.error('**FAILED TO START PLAYBACK**:', playErr.name, playErr.message);
            // Don't skip - try to continue anyway
          }
        };

        // When audio finishes playing
        audioPlayer.onended = () => {
          console.log(`‚úì Sentence ${chunk.sentenceIndex} finished, playing next`);
          resolve();
          playNextAudioChunk();
        };

        // Error handler
        audioPlayer.onerror = (e) => {
          console.error('**AUDIO ELEMENT ERROR**:', audioPlayer.error?.message, 'code:', audioPlayer.error?.code);
          console.error('ReadyState:', audioPlayer.readyState, 'NetworkState:', audioPlayer.networkState);
          // Still try to play next chunk
          resolve();
          playNextAudioChunk();
        };
      });
    }

    // Called when all streaming is done
    function onStreamingComplete() {
      streamingComplete = true;
      console.log('Streaming complete flag set, queue length:', audioChunksQueue.length);

      // If we have buffered chunks but haven't started playing yet, start now
      if (!isPlayingQueue && audioChunksQueue.length > 0) {
        console.log('Streaming done, starting playback with buffered chunks');
        playNextAudioChunk();
      }

      // If no chunks are queued and we're not actively playing, clean up now
      if (audioChunksQueue.length === 0 && !isPlayingQueue) {
        console.log('No chunks left, cleaning up immediately');
        hideSpeechBubble();
        setMouthShape(0);
        status.textContent = '';
        setButtonState('idle');
      }
      // Otherwise, playNextAudioChunk will clean up when queue is empty
    }

    // Clear all lip-sync timeouts
    function clearLipSyncTimeouts() {
      currentLipSyncTimeouts.forEach(t => clearTimeout(t));
      currentLipSyncTimeouts = [];
    }

    // Track displayed sentences for speech bubble (synced with audio playback)
    let displayedSentences = [];

    // Reset streaming playback state
    function resetStreamingPlayback() {
      clearLipSyncTimeouts();
      audioChunksQueue = [];
      isPlayingQueue = false;
      cumulativeDuration = 0;
      streamingComplete = false;
      displayedSentences = [];
      setMouthShape(0);
      audioPlayer.pause();
      audioPlayer.src = '';
    }

    // Parse SSE events from buffer
    function parseSSEBuffer(buffer) {
      const events = [];
      const lines = buffer.split('\n');
      let currentEvent = null;
      let remaining = '';

      for (let i = 0; i < lines.length; i++) {
        const line = lines[i];

        // Check if this might be an incomplete line at the end
        if (i === lines.length - 1 && line !== '') {
          remaining = line;
          break;
        }

        if (line.startsWith('event: ')) {
          currentEvent = { type: line.slice(7).trim(), data: null };
        } else if (line.startsWith('data: ') && currentEvent) {
          try {
            currentEvent.data = JSON.parse(line.slice(6));
            events.push(currentEvent);
          } catch (e) {
            console.warn('Failed to parse SSE data:', line);
          }
          currentEvent = null;
        } else if (line === '' && currentEvent) {
          // Empty line ends event without data
          currentEvent = null;
        }
      }

      return { events, remaining };
    }

    // Send audio to server with polling-based response
    async function sendAudioToServer(audioBlob) {
      try {
        setButtonState('disabled');
        status.textContent = '◊ó◊ï◊©◊ë...';
        startThinkingAnimation();
        resetStreamingPlayback();

        const params = new URLSearchParams({
          mouthShapeCount: MOUTH_SHAPE_COUNT,
          lipSyncMethod: LIP_SYNC_METHOD,
          userId: USER_ID
        });

        // Add kid ID if available - server will fetch full context from Firestore
        if (kidName) {
          params.append('kidId', kidName);
        }

        // Start the session
        console.log('üöÄ Starting polling session...');
        const startResponse = await fetch(`${API_URL}/avatar/converse-poll-start?${params}`, {
          method: 'POST',
          headers: { 'Content-Type': 'audio/webm' },
          body: audioBlob
        });

        if (!startResponse.ok) {
          throw new Error(`Server error: ${startResponse.status}`);
        }

        const { sessionId } = await startResponse.json();
        console.log('‚úì Session started:', sessionId);

        // Poll for updates
        let textByIndex = {}; // Map sentence index to text
        let receivedFirstAudio = false;
        let isComplete = false;
        let receivedSentences = new Set(); // Track which sentences we've received

        while (!isComplete) {
          await new Promise(resolve => setTimeout(resolve, 100)); // Poll every 100ms

          const pollResponse = await fetch(`${API_URL}/avatar/converse-poll?sessionId=${sessionId}`);

          if (!pollResponse.ok) {
            throw new Error(`Poll error: ${pollResponse.status}`);
          }

          const data = await pollResponse.json();

          // Handle transcript
          if (data.transcript && !receivedFirstAudio) {
            console.log('Transcribed:', data.transcript.userText);
            console.log(`Transcription time: ${data.transcript.transcribeTime}ms`);
          }

          // Handle text chunks - store by index, don't display yet (wait for audio sync)
          for (const textChunk of data.textChunks) {
            textByIndex[textChunk.index] = textChunk.text;
            stopThinkingAnimation();
            status.textContent = '';
          }

          // Handle audio chunks
          for (const audioChunk of data.audioChunks) {
            if (!receivedFirstAudio) {
              receivedFirstAudio = true;
              console.log('First audio chunk received');
            }

            // Only queue if we haven't received this sentence yet
            if (!receivedSentences.has(audioChunk.sentenceIndex)) {
              receivedSentences.add(audioChunk.sentenceIndex);
              // Pass the text along with the audio so it displays when audio plays
              const text = textByIndex[audioChunk.sentenceIndex] || '';
              await queueAudioChunk(audioChunk.audioBase64, audioChunk.lipSyncData, audioChunk.sentenceIndex, text);
            }
          }

          // Check if complete
          if (data.isComplete) {
            isComplete = true;
            console.log('Polling complete:', data.metrics);
            onStreamingComplete();
          }

          // Handle error
          if (data.error) {
            throw new Error(data.error);
          }
        }

      } catch (err) {
        console.error('Polling error:', err);
        stopThinkingAnimation();
        resetStreamingPlayback();
        status.textContent = '◊©◊í◊ô◊ê◊î';
        setTimeout(() => {
          status.textContent = '';
          setButtonState('idle');
        }, 2000);
      }
    }

    // Event listeners
    recordBtn.addEventListener('click', toggleRecording);

    // Initialize
    (async () => {
      loadMouthImages();
      loadThinkingImages();
      loadListeningImages();
    })();
  </script>
</body>
</html>
